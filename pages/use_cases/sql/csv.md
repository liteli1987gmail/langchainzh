# CSV

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éå¸¸é€‚åˆæ„å»ºå„ç§æ•°æ®æºä¸Šçš„é—®é¢˜-ç­”æ¡ˆç³»ç»Ÿã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•åœ¨å­˜å‚¨åœ¨CSVæ–‡ä»¶ä¸­çš„æ•°æ®ä¸Šæ„å»ºé—®ç­”ç³»ç»Ÿã€‚ä¸ä½¿ç”¨SQLæ•°æ®åº“ä¸€æ ·ï¼Œä¸CSVæ–‡ä»¶å·¥ä½œçš„å…³é”®ä¹Ÿæ˜¯è®©LLMèƒ½å¤Ÿä½¿ç”¨æŸ¥è¯¢å’Œä¸æ•°æ®äº¤äº’çš„å·¥å…·ã€‚ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•ï¼š

* **æ¨è**ï¼šå°†CSVæ–‡ä»¶åŠ è½½åˆ°SQLæ•°æ®åº“ä¸­ï¼Œå¹¶ä½¿ç”¨[SQLç”¨ä¾‹æ–‡æ¡£](/use_cases/sql/)ä¸­æ¦‚è¿°çš„æ–¹æ³•ã€‚
* è®©LLMè®¿é—®Pythonç¯å¢ƒï¼Œåœ¨é‚£é‡Œå®ƒå¯ä»¥ä½¿ç”¨Pandasç­‰åº“ä¸æ•°æ®äº¤äº’ã€‚

## âš ï¸ å®‰å…¨è¯´æ˜ âš ï¸

ä¸Šè¿°ä¸¤ç§æ–¹æ³•éƒ½å­˜åœ¨æ˜¾è‘—é£é™©ã€‚ä½¿ç”¨SQLéœ€è¦æ‰§è¡Œæ¨¡å‹ç”Ÿæˆçš„SQLæŸ¥è¯¢ã€‚ä½¿ç”¨Pandasè¿™æ ·çš„åº“éœ€è¦è®©æ¨¡å‹æ‰§è¡ŒPythonä»£ç ã€‚ç”±äºé™åˆ¶SQLè¿æ¥æƒé™å’Œæ¸…ç†SQLæŸ¥è¯¢æ¯”æ²™ç®±åŒ–Pythonç¯å¢ƒæ›´å®¹æ˜“ï¼Œ**æˆ‘ä»¬å¼ºçƒˆæ¨èé€šè¿‡SQLä¸CSVæ•°æ®äº¤äº’ã€‚** æœ‰å…³ä¸€èˆ¬å®‰å…¨æœ€ä½³å®è·µçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·[æŸ¥çœ‹æ­¤å¤„](/docs/security)ã€‚

## è®¾ç½®
æœ¬æŒ‡å—çš„ä¾èµ–é¡¹ï¼š

```python
%pip install -qU langchain langchain-openai langchain-community langchain-experimental pandas
```

è®¾ç½®æ‰€éœ€çš„ç¯å¢ƒå˜é‡ï¼š


```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Using LangSmith is recommended but not required. Uncomment below lines to use.
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```

Download the [Titanic dataset](https://www.kaggle.com/datasets/yasserh/titanic-dataset) if you don't already have it:


```python
!wget https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv -O titanic.csv
```


```python
import pandas as pd

df = pd.read_csv("titanic.csv")
print(df.shape)
print(df.columns.tolist())
```

    (887, 8)
    ['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']
    

## SQL

ä½¿ç”¨SQLä¸CSVæ•°æ®äº¤äº’æ˜¯æ¨èçš„æ–¹æ³•ï¼Œå› ä¸ºä¸ä»»æ„Pythonä»£ç ç›¸æ¯”ï¼Œé™åˆ¶æƒé™å’ŒéªŒè¯SQLæŸ¥è¯¢æ›´åŠ å®¹æ˜“ã€‚

å¤§å¤šæ•°SQLæ•°æ®åº“éƒ½å¯ä»¥å°†CSVæ–‡ä»¶è½»æ¾åŠ è½½ä¸ºè¡¨æ ¼ï¼ˆ[DuckDB](https://duckdb.org/docs/data/csv/overview.html)ï¼Œ[SQLite](https://www.sqlite.org/csv.html)ï¼Œç­‰ç­‰ï¼‰ã€‚ä¸€æ—¦å®Œæˆæ­¤æ“ä½œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨SQLç”¨ä¾‹æŒ‡å—ä¸­æ¦‚è¿°çš„æ‰€æœ‰è¿é”å’Œäº§ç”Ÿä»£ç†çš„æŠ€æœ¯ã€‚ä¸‹é¢æ˜¯ä½¿ç”¨SQLiteçš„å¿«é€Ÿç¤ºä¾‹ï¼š


```python
from langchain_community.utilities import SQLDatabase
from sqlalchemy import create_engine

engine = create_engine("sqlite:///titanic.db")
df.to_sql("titanic", engine, index=False)
```




    887




```python
db = SQLDatabase(engine=engine)
print(db.dialect)
print(db.get_usable_table_names())
db.run("SELECT * FROM titanic WHERE Age < 2;")
```

    sqlite
    ['titanic']
    




    "[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]"



å¹¶åˆ›å»ºä¸€ä¸ª[SQLä»£ç†](/use_cases/sql/agents)ä¸ä¹‹äº¤äº’ï¼š


```python
from langchain_community.agent_toolkits import create_sql_agent
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
agent_executor = create_sql_agent(llm, db=db, agent_type="openai-tools", verbose=True)
```


```python
agent_executor.invoke({"input": "what's the average age of survivors"})
```

    
    
    [1m> è¿›å…¥æ–°çš„AgentExecutoré“¾...[0m
    [32;1m[1;3m
    Invoking: `sql_db_list_tables` with `{}`
    
    
    [0m[38;5;200m[1;3mtitanic[0m[32;1m[1;3m
    Invoking: `sql_db_schema` with `{'table_names': 'titanic'}`
    
    
    [0m[33;1m[1;3m
    CREATE TABLE titanic (
    	"Survived" BIGINT, 
    	"Pclass" BIGINT, 
    	"Name" TEXT, 
    	"Sex" TEXT, 
    	"Age" FLOAT, 
    	"Siblings/Spouses Aboard" BIGINT, 
    	"Parents/Children Aboard" BIGINT, 
    	"Fare" FLOAT
    )
    
    /*
    3 rows from titanic table:
    Survived	Pclass	Name	Sex	Age	Siblings/Spouses Aboard	Parents/Children Aboard	Fare
    0	3	Mr. Owen Harris Braund	male	22.0	1	0	7.25
    1	1	Mrs. John Bradley (Florence Briggs Thayer) Cumings	female	38.0	1	0	71.2833
    1	3	Miss. Laina Heikkinen	female	26.0	0	0	7.925
    */[0m[32;1m[1;3m
    Invoking: `sql_db_query` with `{'query': 'SELECT AVG(Age) AS AverageAge FROM titanic WHERE Survived = 1'}`
    responded: è¦æ‰¾åˆ°å¹¸å­˜è€…çš„å¹³å‡å¹´é¾„ï¼Œæˆ‘å°†æŸ¥è¯¢â€œtitanicâ€è¡¨ï¼Œå¹¶è®¡ç®—â€œSurvivedâ€ç­‰äº1çš„è¡Œçš„â€œAgeâ€åˆ—çš„å¹³å‡å€¼ã€‚
    
     ä»¥ä¸‹æ˜¯SQLæŸ¥è¯¢è¯­å¥ï¼š
    
    ```sql
    SELECT AVG(Age) AS AverageAge
    FROM titanic
    WHERE Survived = 1
    ```
    
    æ‰§è¡Œæ­¤æŸ¥è¯¢å°†ç»™å‡ºå¹¸å­˜è€…çš„å¹³å‡å¹´é¾„ã€‚
    
    [0m[36;1m[1;3m[(28.408391812865496,)][0m[32;1m[1;3må¹¸å­˜è€…çš„å¹³å‡å¹´é¾„çº¦ä¸º28.41å²ã€‚[0m
    
    [1m> é“¾ç»“æŸã€‚[0m
    




    {'input': "what's the average age of survivors",
     'output': 'å¹¸å­˜è€…çš„å¹³å‡å¹´é¾„çº¦ä¸º28.41å²ã€‚'}



æ­¤æ–¹æ³•å¾ˆå®¹æ˜“æ‰©å±•åˆ°å¤šä¸ªCSVæ–‡ä»¶ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ªæ–‡ä»¶åŠ è½½åˆ°æˆ‘ä»¬çš„æ•°æ®åº“ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„è¡¨ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…[SQLæŒ‡å—](/use_cases/sql/)ã€‚

## Pandas

é™¤äº†SQLï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨è¯¸å¦‚Pandasä¹‹ç±»çš„æ•°æ®åˆ†æåº“å’ŒLLMsçš„ä»£ç ç”Ÿæˆèƒ½åŠ›ä¸CSVæ•°æ®è¿›è¡Œäº¤äº’ã€‚åŒæ ·ï¼Œ**åœ¨æ²¡æœ‰å»ºç«‹å……åˆ†å®‰å…¨ä¿éšœçš„æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•ä¸é€‚ç”¨äºç”Ÿäº§ç¯å¢ƒ**ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ä»£ç æ‰§è¡Œå®ç”¨ç¨‹åºå’Œæ„é€ å‡½æ•°ä½äº`langchain-experimental`åŒ…ä¸­ã€‚

### Chain

å¤§å¤šæ•°LLMsç»è¿‡è¶³å¤Ÿçš„è®­ç»ƒï¼Œå¯ä»¥é€šè¿‡è¢«è¦æ±‚äº§ç”Ÿå®ƒæ¥ç”ŸæˆPandasçš„Pythonä»£ç ï¼š


```python
ai_msg = llm.invoke(
    "æˆ‘æœ‰ä¸€ä¸ªåä¸º'df'çš„pandas DataFrameï¼Œå…·æœ‰'Age'å’Œ'Fare'åˆ—ã€‚ç¼–å†™ä»£ç è®¡ç®—ä¸¤åˆ—ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åªè¿”å›Pythonä»£ç çš„Markdownç‰‡æ®µï¼Œä¸è¿”å›å…¶ä»–ä»»ä½•å†…å®¹ã€‚"
)
print(ai_msg.content)
```

    ```python
    correlation = df['Age'].corr(df['Fare'])
    correlation
    ```
    

æˆ‘ä»¬å¯ä»¥å°†æ­¤åŠŸèƒ½ä¸Pythonæ‰§è¡Œå·¥å…·ç»“åˆä½¿ç”¨ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®åˆ†æé“¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å¸Œæœ›å°†CSVè¡¨åŠ è½½ä¸ºæ•°æ®å¸§ï¼Œå¹¶ä½¿å·¥å…·èƒ½å¤Ÿè®¿é—®æ­¤æ•°æ®å¸§ï¼š


```python
import pandas as pd
from langchain_core.prompts import ChatPromptTemplate
from langchain_experimental.tools import PythonAstREPLTool

df = pd.read_csv("titanic.csv")
tool = PythonAstREPLTool(locals={"df": df})
tool.invoke("df['Fare'].mean()")
```




    32.30542018038331



ä¸ºäº†å¸®åŠ©å¼ºåˆ¶æ‰§è¡Œæˆ‘ä»¬çš„Pythonå·¥å…·çš„æ­£ç¡®ä½¿ç”¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[å‡½æ•°è°ƒç”¨](/modules/model_io/chat/function_calling)ï¼š


```python
llm_with_tools = llm.bind_tools([tool], tool_choice=tool.name)
llm_with_tools.invoke(
    "æˆ‘æœ‰ä¸€ä¸ªåä¸º'df'çš„æ•°æ®å¸§ï¼Œæƒ³è¦çŸ¥é“'Age'å’Œ'Fare'åˆ—ä¹‹é—´çš„ç›¸å…³æ€§"
)
```




    AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6TZsNaCqOcbP7lqWudosQTd6', 'function': {'arguments': '{\n  "query": "df[[\'Age\', \'Fare\']].corr()"\n}', 'name': 'python_repl_ast'}, 'type': 'function'}]})



æˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ª[OpenAIå·¥å…·è¾“å‡ºè§£æå™¨](/modules/model_io/output_parsers/types/openai_tools)å°†å‡½æ•°è°ƒç”¨æå–ä¸ºå­—å…¸ï¼š


```python
from langchain.output_parsers.openai_tools import JsonOutputKeyToolsParser

parser = JsonOutputKeyToolsParser(tool.name, first_tool_only=True)
(llm_with_tools | parser).invoke(
    "æˆ‘æœ‰ä¸€ä¸ªåä¸º'df'çš„æ•°æ®å¸§ï¼Œæƒ³è¦çŸ¥é“'Age'å’Œ'Fare'åˆ—ä¹‹é—´çš„ç›¸å…³æ€§"
)
```




    {'query': "df[['Age', 'Fare']].corr()"}



ç„¶åï¼Œæˆ‘ä»¬å°†ä¸ä¸€ä¸ªæç¤ºç»„åˆï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åªæŒ‡å®šä¸€ä¸ªé—®é¢˜ï¼Œè€Œæ— éœ€åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æŒ‡å®šæ•°æ®å¸§ä¿¡æ¯ï¼š


```python
system = f"""æ‚¨å¯ä»¥è®¿é—®ä¸€ä¸ªåä¸º'df'çš„pandasæ•°æ®å¸§ã€‚\
è¿™æ˜¯'df.head().to_markdown()'çš„è¾“å‡ºï¼š

```
{df.head().to_markdown()}
```

ç»™å®šä¸€ä¸ªç”¨æˆ·é—®é¢˜ï¼Œç¼–å†™å›ç­”é—®é¢˜çš„Pythonä»£ç ã€‚\
åªè¿”å›æœ‰æ•ˆçš„Pythonä»£ç ï¼Œä¸è¿”å›å…¶ä»–ä»»ä½•å†…å®¹ã€‚\
ä¸è¦å‡è®¾ä½ å¯ä»¥è®¿é—®é™¤å†…ç½®çš„Pythonåº“å’Œpandasä¹‹å¤–çš„ä»»ä½•åº“ã€‚"""
prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{question}")])
code_chain = prompt | llm_with_tools | parser
code_chain.invoke({"question": "å¹´é¾„å’Œè´¹ç”¨ä¹‹é—´çš„ç›¸å…³æ€§æ˜¯å¤šå°‘"})
```




    {'query': "df[['Age', 'Fare']].corr()"}



æœ€åï¼Œæˆ‘ä»¬å°†æ·»åŠ æˆ‘ä»¬çš„Pythonå·¥å…·ï¼Œä»¥ä¾¿æ‰§è¡Œç”Ÿæˆçš„ä»£ç ï¼š


```python
chain = prompt | llm_with_tools | parser | tool  # noqa
chain.invoke({"question": "å¹´é¾„å’Œè´¹ç”¨ä¹‹é—´çš„ç›¸å…³æ€§æ˜¯å¤šå°‘"})
```




    0.11232863699941621



å°±è¿™æ ·ï¼Œæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªç®€å•çš„æ•°æ®åˆ†æé“¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹LangSmithè·Ÿè¸ªæ¥æŸ¥çœ‹ä¸­é—´æ­¥éª¤ï¼šhttps://smith.langchain.com/public/b1309290-7212-49b7-bde2-75b39a32b49a/r

æˆ‘ä»¬å¯ä»¥åœ¨æœ€åæ·»åŠ å¦ä¸€ä¸ªLLMè°ƒç”¨æ¥ç”Ÿæˆå¯¹è¯å›å¤ï¼Œè¿™æ ·æˆ‘ä»¬ä¸ä»…ä»…ä¼šç”¨å·¥å…·çš„è¾“å‡ºæ¥å›åº”ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†åœ¨æç¤ºä¸­æ·»åŠ ä¸€ä¸ªèŠå¤©å†å²è®°å½•`MessagesPlaceholder`ï¼š


```python
from operator import itemgetter

from langchain_core.messages import ToolMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough

system = f"""æ‚¨å¯ä»¥è®¿é—®ä¸€ä¸ªåä¸º'df'çš„pandasæ•°æ®å¸§ã€‚\
è¿™æ˜¯'df.head().to_markdown()'çš„è¾“å‡ºï¼š

```
{df.head().to_markdown()}
```

ç»™å®šä¸€ä¸ªç”¨æˆ·é—®é¢˜ï¼Œç¼–å†™å›ç­”é—®é¢˜çš„Pythonä»£ç ã€‚\
ä¸è¦å‡è®¾ä½ å¯ä»¥è®¿é—®é™¤å†…ç½®çš„Pythonåº“å’Œpandasä¹‹å¤–çš„ä»»ä½•åº“ã€‚\
ä¸€æ—¦æ‚¨æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥å›ç­”é—®é¢˜ã€‚"""
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            system,
        ),
        ("human", "{question}"),
>>>>>>> fix typo
    ]
)
code_chain = prompt | llm_with_tools | parser | tool  # noqa
chain = code_chain | MessagesPlaceholder()
conversation = [("human", "What's the correlation between age and fare?")]
for index, (role, message) in enumerate(conversation):
    if index == len(conversation) - 1:
        output = chain.invoke({"question": message})
    else:
        output = RunnablePassthrough(role=role, content=message)
    chain = chain | output | StrOutputParser() | itemgetter("output")
    print(f"{role}: {message}")
    print("Bot:", chain.invoke({}))
    print()
```

ä»¥ä¸Šå°±æ˜¯ä½¿ç”¨Pandasè¿›è¡Œæ•°æ®åˆ†æçš„ä¸€ä¸ªç®€å•ç¤ºä¾‹ã€‚è¯·æ³¨æ„ï¼Œæ‰§è¡Œç”Ÿæˆçš„ä»£ç å¯èƒ½å­˜åœ¨å®‰å…¨é£é™©ï¼Œè¯·åŠ¡å¿…éªŒè¯å’Œå®¡æŸ¥ç”Ÿæˆçš„ä»£ç ã€‚


```python
# This MessagesPlaceholder allows us to optionally append an arbitrary number of messages
# at the end of the prompt using the 'chat_history' arg.
MessagesPlaceholder("chat_history", optional=True),
]
)


def _get_chat_history(x: dict) -> list:
"""Parse the chain output up to this point into a list of chat history messages to insert in the prompt."""
ai_msg = x["ai_msg"]
tool_call_id = x["ai_msg"].additional_kwargs["tool_calls"][0]["id"]
tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(x["tool_output"]))
return [ai_msg, tool_msg]


chain = (
RunnablePassthrough.assign(ai_msg=prompt | llm_with_tools)
.assign(tool_output=itemgetter("ai_msg") | parser | tool)
.assign(chat_history=_get_chat_history)
.assign(response=prompt | llm | StrOutputParser())
.pick(["tool_output", "response"])
)
```


```python
chain.invoke({"question": "What's the correlation between age and fare"})
```




    {'tool_output': 0.11232863699941621,
    'response': 'The correlation between age and fare is approximately 0.112.'}


LangSmithè¿½è¸ªè½¨è¿¹: [https://smith.langchain.com/public/ca689f8a-5655-4224-8bcf-982080744462/r](https://smith.langchain.com/public/ca689f8a-5655-4224-8bcf-982080744462/r)


### ä»£ç†

å¯¹äºå¤æ‚çš„é—®é¢˜ï¼Œè®©LLMèƒ½å¤Ÿåœ¨ä¿æŒå…ˆå‰æ‰§è¡Œçš„è¾“å…¥å’Œè¾“å‡ºçš„æƒ…å†µä¸‹è¿­ä»£æ‰§è¡Œä»£ç æ˜¯éå¸¸æœ‰å¸®åŠ©çš„ã€‚è¿™å°±æ˜¯ä»£ç†çš„ä½œç”¨æ‰€åœ¨ã€‚å®ƒä»¬å…è®¸LLMå†³å®šéœ€è¦è°ƒç”¨å·¥å…·çš„æ¬¡æ•°ï¼Œå¹¶è·Ÿè¸ªåˆ°ç›®å‰ä¸ºæ­¢çš„æ‰§è¡Œæƒ…å†µã€‚[create_pandas_dataframe_agent](https://api.python.langchain.com/en/latest/agents/langchain_experimental.agents.agent_toolkits.pandas.base.create_pandas_dataframe_agent.html) æ˜¯ä¸€ä¸ªå†…ç½®ä»£ç†ï¼Œä½¿å¤„ç†æ•°æ®æ¡†å˜å¾—å®¹æ˜“ï¼š


```python
from langchain_experimental.agents import create_pandas_dataframe_agent

agent = create_pandas_dataframe_agent(llm, df, agent_type="openai-tools", verbose=True)
agent.invoke(
{
"input": "What's the correlation between age and fare? is that greater than the correlation between fare and survival?"
}
)
```



[é”™è¯¯] ä»£ç†çš„ agent_type 'openai-tools' ä¸æ­£ç¡®ã€‚è¯·æä¾›æ­£ç¡®çš„ agent_typeã€‚

LangSmithè¿½è¸ªè½¨è¿¹: [https://smith.langchain.com/public/8e6c23cc-782c-4203-bac6-2a28c770c9f0/r](https://smith.langchain.com/public/8e6c23cc-782c-4203-bac6-2a28c770c9f0/r)

### å¤šä¸ª CSV

è¦å¤„ç†å¤šä¸ª CSVï¼ˆæˆ–æ•°æ®æ¡†ï¼‰ï¼Œæˆ‘ä»¬åªéœ€å°†å¤šä¸ªæ•°æ®æ¡†ä¼ é€’ç»™æˆ‘ä»¬çš„ Python å·¥å…·ã€‚æˆ‘ä»¬çš„ `create_pandas_dataframe_agent` æ„é€ å‡½æ•°å¯ä»¥å¼€ç®±å³ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’ä¸€ä¸ªæ•°æ®æ¡†åˆ—è¡¨ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªã€‚å¦‚æœæˆ‘ä»¬è‡ªå·±æ„å»ºä¸€ä¸ªé“¾ï¼Œå¯ä»¥è¿™æ ·åšï¼š

```python
df_1 = df[["Age", "Fare"]]
df_2 = df[["Fare", "Survived"]]

tool = PythonAstREPLTool(locals={"df_1": df_1, "df_2": df_2})
llm_with_tool = llm.bind_tools(tools=[tool], tool_choice=tool.name)
df_template = "```python\n{df_name}.head().to_markdown()\n>>> {df_head}\n```"
df_context = "\n\n".join(
df_template.format(df_head=_df.head().to_markdown(), df_name=df_name)
for _df, df_name in [(df_1, "df_1"), (df_2, "df_2")]
)

system = f"You have access to a number of pandas dataframes. \
Here is a sample of rows from each dataframe and the python code that was used to generate the sample:\n\n{df_context}\n\nGiven a user question about the dataframes, write the Python code to answer it. \
Don't assume you have access to any libraries other than built-in Python ones and pandas. \
Make sure to refer only to the variables mentioned above."
prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{question}")])

chain = prompt | llm_with_tool | parser | tool
chain.invoke(
{
"question": "return the difference in the correlation between age and fare and the correlation between fare and survival"
}
)
```

-0.14384991262954416

LangSmithè¿½è¸ªè½¨è¿¹: [https://smith.langchain.com/public/653e499f-179c-4757-8041-f5e2a5f11fcc/r](https://smith.langchain.com/public/653e499f-179c-4757-8041-f5e2a5f11fcc/r)

### æ²™ç›’ä»£ç æ‰§è¡Œ

æœ‰è®¸å¤šå·¥å…·å¦‚ [E2B](/docs/integrations/tools/e2b_data_analysis) å’Œ [Bearly](/docs/integrations/tools/bearly) æä¾›äº†ç”¨äº Python ä»£ç æ‰§è¡Œçš„æ²™ç›’ç¯å¢ƒï¼Œä»¥å…è®¸æ›´å®‰å…¨çš„ä»£ç æ‰§è¡Œé“¾å’Œä»£ç†ã€‚

## ä¸‹ä¸€æ­¥

å¯¹äºæ›´é«˜çº§çš„æ•°æ®åˆ†æåº”ç”¨ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹ï¼š

* [SQL ç”¨ä¾‹](/use_cases/sql/): å¤„ç† SQL æ•°æ®åº“å’Œ CSV æ–‡ä»¶çš„è®¸å¤šæŒ‘æˆ˜å¯¹ä»»ä½•ç»“æ„åŒ–æ•°æ®ç±»å‹éƒ½æ˜¯é€šç”¨çš„ï¼Œå› æ­¤å³ä½¿æ‚¨ä½¿ç”¨ Pandas è¿›è¡Œ CSV æ•°æ®åˆ†æï¼Œä¹Ÿæœ‰å¿…è¦é˜…è¯» SQL æŠ€æœ¯ã€‚
* [å·¥å…·ä½¿ç”¨](/use_cases/tool_use/): æ¶‰åŠé“¾å’Œä»£ç†è°ƒç”¨å·¥å…·æ—¶çš„ä¸€èˆ¬æœ€ä½³å®è·µæŒ‡å—ã€‚
* [ä»£ç†](/modules/agents/): äº†è§£æ„å»º LLM ä»£ç†çš„åŸºç¡€çŸ¥è¯†ã€‚
* é›†æˆï¼šæ²™ç›’ç¯å¢ƒå¦‚ [E2B](/docs/integrations/tools/e2b_data_analysis) å’Œ [Bearly](/docs/integrations/tools/bearly)ï¼Œå®ç”¨å·¥å…·å¦‚ [SQLDatabase](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.sql_database.SQLDatabase.html#langchain_community.utilities.sql_database.SQLDatabase)ï¼Œç›¸å…³ä»£ç†å¦‚ [Spark DataFrame agent](/docs/integrations/toolkits/spark)ã€‚

