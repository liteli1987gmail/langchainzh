# æµå¼è¾“å‡º

æµå¼è¾“å‡ºæ˜¯LLMåº”ç”¨çš„é‡è¦ç”¨æˆ·ä½“éªŒè€ƒè™‘å› ç´ ï¼Œä»£ç†ä¹Ÿä¸ä¾‹å¤–ã€‚ä¸ä»£ç†ä¸€èµ·æµåª’ä½“ä¼šæ›´åŠ å¤æ‚ï¼Œå› ä¸ºä½ ä¸ä»…æƒ³è¦æµå¼ä¼ è¾“æœ€ç»ˆç­”æ¡ˆçš„ä»¤ç‰Œï¼Œè¿˜å¯èƒ½å¸Œæœ›æµå¼ä¼ è¾“ä»£ç†æ‰€é‡‡å–çš„ä¸­é—´æ­¥éª¤ã€‚

åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ç”¨äºæµå¼ä¼ è¾“çš„`stream/astream`å’Œ`astream_events`ã€‚

æˆ‘ä»¬çš„ä»£ç†å°†ä½¿ç”¨å·¥å…·APIè¿›è¡Œå·¥å…·è°ƒç”¨ï¼š

1. `where_cat_is_hiding`: è¿”å›çŒ«è—åœ¨å“ªé‡Œçš„ä½ç½®
2. `get_items`: åˆ—å‡ºå¯ä»¥åœ¨ç‰¹å®šåœ°æ–¹æ‰¾åˆ°çš„ç‰©å“

è¿™äº›å·¥å…·å°†å…è®¸æˆ‘ä»¬åœ¨ä¸€ä¸ªæ›´æœ‰è¶£çš„æƒ…å†µä¸‹æ¢ç´¢æµå¼ä¼ è¾“ï¼Œä»£ç†å°†ä¸å¾—ä¸ä½¿ç”¨ä¸¤è€…æ¥å›ç­”ä¸€äº›é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œå›ç­”é—®é¢˜`çŒ«è—åœ¨å“ªé‡Œçš„ç‰©å“ï¼Ÿ`ï¼‰ã€‚

å‡†å¤‡å¥½äº†å—ï¼ŸğŸï¸


```python
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate
from langchain.tools import tool
from langchain_core.callbacks import Callbacks
from langchain_openai import ChatOpenAI
```

## åˆ›å»ºæ¨¡å‹

**æ³¨æ„** æˆ‘ä»¬åœ¨LLMä¸Šè®¾ç½®äº†`streaming=True`ã€‚è¿™å°†å…è®¸æˆ‘ä»¬ä½¿ç”¨`astream_events` APIä»ä»£ç†ä¸­æµå¼ä¼ è¾“ä»¤ç‰Œã€‚è¿™å¯¹äºLangChainçš„æ—§ç‰ˆæœ¬æ˜¯å¿…éœ€çš„ã€‚


```python
model = ChatOpenAI(temperature=0, streaming=True)
```

## å·¥å…·

æˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªä¾èµ–äºèŠå¤©æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„å·¥å…·ï¼


```python
import random


@tool
async def where_cat_is_hiding() -> str:
    """çŒ«ç°åœ¨èº²åœ¨å“ªé‡Œï¼Ÿ"""
    return random.choice(["åºŠä¸‹", "æ¶å­ä¸Š"])


@tool
async def get_items(place: str) -> str:
    """ä½¿ç”¨æ­¤å·¥å…·æŸ¥æ‰¾ç»™å®šåœ°æ–¹æœ‰å“ªäº›ç‰©å“ã€‚"""
    if "åºŠ" in place:  # å¯¹äºåºŠä¸‹
        return "è¢œå­ã€é‹å­å’Œç°å°˜å…”"
    if "æ¶å­" in place:  # å¯¹äº'æ¶å­'
        return "ä¹¦ã€é“…ç¬”å’Œå›¾ç‰‡"
    else:  # å¦‚æœä»£ç†å†³å®šè¯¢é—®ä¸åŒçš„åœ°æ–¹
        return "çŒ«é›¶é£Ÿ"
```


```python
await where_cat_is_hiding.ainvoke({})
```




    'æ¶å­ä¸Š'




```python
await get_items.ainvoke({"place": "æ¶å­"})
```




    'ä¹¦ã€é“…ç¬”å’Œå›¾ç‰‡'



## åˆå§‹åŒ–ä»£ç†

è¿™é‡Œï¼Œæˆ‘ä»¬å°†åˆå§‹åŒ–ä¸€ä¸ªOpenAIå·¥å…·ä»£ç†ã€‚

**æ³¨æ„** è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨`"run_name"="Agent"`å°†åç§°`Agent`ä¸æˆ‘ä»¬çš„ä»£ç†å…³è”èµ·æ¥ã€‚æˆ‘ä»¬ç¨åå°†åœ¨`astream_events` APIä¸­ä½¿ç”¨è¿™ä¸€äº‹å®ã€‚


```python
# è·å–è¦ä½¿ç”¨çš„æç¤º - æ‚¨å¯ä»¥ä¿®æ”¹è¿™ä¸ªï¼
prompt = hub.pull("hwchase17/openai-tools-agent")
# æ‰“å°(prompt.messages) -- æŸ¥çœ‹æç¤º
tools = [get_items, where_cat_is_hiding]
agent = create_openai_tools_agent(
    model.with_config({"tags": ["agent_llm"]}), tools, prompt
)
agent_executor = AgentExecutor(agent=agent, tools=tools).with_config(
    {"run_name": "Agent"}
)
```

## æµä¼ ä¸­é—´æ­¥éª¤

æˆ‘ä»¬å°†ä½¿ç”¨AgentExecutorçš„`.stream`æ–¹æ³•æ¥æµå¼ä¼ è¾“ä»£ç†çš„ä¸­é—´æ­¥éª¤ã€‚

`.stream`çš„è¾“å‡ºåœ¨æ“ä½œå’Œè§‚å¯Ÿå¯¹ä¹‹é—´äº¤æ›¿ï¼Œæœ€ç»ˆä»¥ä»£ç†æ˜¯å¦è¾¾åˆ°ç›®æ ‡ä¸ºç»“æŸã€‚

å®ƒçœ‹èµ·æ¥åƒè¿™æ ·ï¼š

1. æ“ä½œè¾“å‡º
2. è§‚å¯Ÿè¾“å‡º
3. æ“ä½œè¾“å‡º
4. è§‚å¯Ÿè¾“å‡º

**...ï¼ˆç»§ç»­ç›´åˆ°è¾¾åˆ°ç›®æ ‡ï¼‰...**

ç„¶åï¼Œå¦‚æœè¾¾åˆ°æœ€ç»ˆç›®æ ‡ï¼Œä»£ç†å°†è¾“å‡º**æœ€ç»ˆç­”æ¡ˆ**ã€‚


è¿™äº›è¾“å‡ºçš„å†…å®¹æ‘˜è¦å¦‚ä¸‹ï¼š

| è¾“å‡º              | å†…å®¹                                                                                       |
|----------------------|--------------------------------------------------------------------------------------|
| **æ“ä½œ**     | `actions` `AgentAction`æˆ–å­ç±»ï¼Œä¸æ“ä½œè°ƒç”¨å¯¹åº”çš„èŠå¤©æ¶ˆæ¯                                                                         |
| **è§‚å¯Ÿ**     | `steps`åˆ°ç›®å‰ä¸ºæ­¢ä»£ç†çš„å†å²è®°å½•ï¼ŒåŒ…æ‹¬å½“å‰æ“ä½œåŠå…¶è§‚å¯Ÿï¼Œ`messages`å…·æœ‰å‡½æ•°è°ƒç”¨ç»“æœçš„èŠå¤©æ¶ˆæ¯ï¼ˆå³è§‚å¯Ÿï¼‰ |
| **æœ€ç»ˆç­”æ¡ˆ**  | `output` `AgentFinish`ï¼Œå…·æœ‰æœ€ç»ˆè¾“å‡ºçš„èŠå¤©æ¶ˆæ¯|


```python
# æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨`pprint`ä»…æ‰“å°åˆ°æ·±åº¦1ï¼Œè¿™æ ·å¯ä»¥æ›´å®¹æ˜“åœ°çœ‹åˆ°é«˜å±‚æ¬¡çš„è¾“å‡ºï¼Œç„¶åå†è¿›è¡ŒæŒ–æ˜ã€‚
import pprint

chunks = []

async for chunk in agent_executor.astream(
    {"input": "çŒ«èº²åœ¨å“ªé‡Œæœ‰å“ªäº›ç‰©å“ï¼Ÿ"}
):
    chunks.append(chunk)
    print("------")
    pprint.pprint(chunk, depth=1)
```

    ------
    {'actions': [...], 'messages': [...]}
    ------
    {'messages': [...], 'steps': [...]}
    ------
    {'actions': [...], 'messages': [...]}
    ------
    {'messages': [...], 'steps': [...]}
    ------
    {'messages': [...],
     'output': 'çŒ«è—åœ¨æ¶å­ä¸Šçš„ç‰©å“æ˜¯ä¹¦ã€é“…ç¬”å’Œå›¾ç‰‡ã€‚'}
    

### ä½¿ç”¨æ¶ˆæ¯

æ‚¨å¯ä»¥è®¿é—®è¾“å‡ºä¸­çš„åŸºç¡€`æ¶ˆæ¯`ã€‚åœ¨å¤„ç†èŠå¤©åº”ç”¨ç¨‹åºæ—¶ï¼Œä½¿ç”¨æ¶ˆæ¯å¯èƒ½ä¼šå¾ˆå¥½ï¼Œå› ä¸ºä¸€åˆ‡éƒ½æ˜¯æ¶ˆæ¯ï¼


```python
chunks[0]["actions"]
```




    [OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\næ­£åœ¨è°ƒç”¨ï¼šä½¿ç”¨`{}`æŸ¥æ‰¾çŒ«çš„è—èº«ä¹‹å¤„\n\n\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_pKy4OLcBx6pR6k3GHBOlH68r', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_pKy4OLcBx6pR6k3GHBOlH68r')]




```python
for chunk in chunks:
    print(chunk["messages"])
```

    [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_pKy4OLcBx6pR6k3GHBOlH68r', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})]
    [FunctionMessage(content='æ¶å­ä¸Š', name='where_cat_is_hiding')]
    [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_qZTz1mRfCCXT18SUy0E07eS4', 'function': {'arguments': '{\n  "place": "æ¶å­"\n}', 'name': 'get_items'}, 'type': 'function'}]})]
    [FunctionMessage(content='ä¹¦ã€é“…ç¬”å’Œå›¾ç‰‡', name='get_items')]
    [AIMessage(content='çŒ«è—åœ¨æ¶å­ä¸Šçš„ç‰©å“æ˜¯ä¹¦ã€é“…ç¬”å’Œå›¾ç‰‡ã€‚')]
    

æ­¤å¤–ï¼Œå®ƒä»¬åŒ…å«å®Œæ•´çš„æ—¥å¿—ä¿¡æ¯ï¼ˆ`actions`å’Œ`steps`ï¼‰ï¼Œå¯èƒ½æ›´å®¹æ˜“åœ¨æ¸²æŸ“ç›®çš„ä¸­å¤„ç†ã€‚

### ä½¿ç”¨AgentAction/Observation

è¾“å‡ºè¿˜åŒ…å«æ›´ä¸°å¯Œç»“æ„åŒ–ä¿¡æ¯ï¼Œä½äº`actions`å’Œ`steps`å†…éƒ¨ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œä½†ä¹Ÿå¯èƒ½æ›´éš¾è§£æã€‚

**æ³¨æ„** `AgentFinish`ä¸ä½œä¸º`streaming`æ–¹æ³•çš„ä¸€éƒ¨åˆ†æä¾›ã€‚å¦‚æœæ‚¨å¸Œæœ›æ·»åŠ è¿™ä¸ªåŠŸèƒ½ï¼Œè¯·åœ¨githubä¸Šå‘èµ·è®¨è®ºå¹¶è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦å®ƒã€‚


```python
async for chunk in agent_executor.astream(
    {"input": "çŒ«èº²åœ¨å“ªé‡Œæœ‰å“ªäº›ç‰©å“ï¼Ÿ"}
):
# ä»£ç†æ‰§è¡Œ

å¦‚æœ`chunk`ä¸­åŒ…å«"actions"å­—æ®µ:
å¯¹äº`chunk["actions"]`ä¸­çš„æ¯ä¸€ä¸ªaction:
è¾“å‡º`Calling Tool: `{action.tool}` with input `{action.tool_input}``

å¦‚æœ`chunk`ä¸­åŒ…å«"steps"å­—æ®µ:
å¯¹äº`chunk["steps"]`ä¸­çš„æ¯ä¸€ä¸ªstep:
è¾“å‡º`Tool Result: `{step.observation}``

å¦‚æœ`chunk`ä¸­åŒ…å«"output"å­—æ®µ:
è¾“å‡º`Final Output: {chunk["output"]}`
å¦åˆ™:
æŠ›å‡ºæ•°å€¼é”™è¯¯

è¾“å‡º"---"åˆ†å‰²çº¿
```

```python
Calling Tool: `where_cat_is_hiding` with input `{}`
---
Tool Result: `on the shelf`
---
Calling Tool: `get_items` with input `{'place': 'shelf'}`
---
Tool Result: `books, pencils and pictures`
---
Final Output: The items located where the cat is hiding on the shelf are books, pencils, and pictures.
---
```

## ä½¿ç”¨è‡ªå®šä¹‰äº‹ä»¶è¿›è¡Œæµå¼å¤„ç†

å¦‚æœé»˜è®¤çš„*stream*è¡Œä¸ºä¸é€‚ç”¨äºä½ çš„åº”ç”¨ç¨‹åº(ä¾‹å¦‚ï¼Œå¦‚æœä½ éœ€è¦ä»ä»£ç†æˆ–å·¥å…·å†…éƒ¨å‘ç”Ÿçš„æ­¥éª¤ä¸­æµå¼å¤„ç†æ¯ä¸ªæ ‡è®°)ï¼Œè¯·ä½¿ç”¨`astream_events` APIã€‚

âš ï¸ è¿™æ˜¯ä¸€ä¸ª**beta**APIï¼Œè¿™æ„å‘³ç€ä¸€äº›ç»†èŠ‚å¯èƒ½ä¼šæ ¹æ®ä½¿ç”¨æƒ…å†µç¨å¾®æ”¹å˜ã€‚
âš ï¸ ä¸ºäº†ç¡®ä¿æ‰€æœ‰å›è°ƒæ­£ç¡®å·¥ä½œï¼Œè¯·å§‹ç»ˆä½¿ç”¨`async`ä»£ç ã€‚å°½é‡é¿å…æ··åˆåŒæ­¥ç‰ˆæœ¬çš„ä»£ç (ä¾‹å¦‚ï¼ŒåŒæ­¥ç‰ˆæœ¬çš„å·¥å…·)ã€‚

æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªAPIæ¥æµå¼å¤„ç†ä»¥ä¸‹äº‹ä»¶:

1. å¸¦è¾“å…¥çš„ä»£ç†å¼€å§‹
2. å¸¦è¾“å…¥çš„å·¥å…·å¼€å§‹
3. å¸¦è¾“å‡ºçš„å·¥å…·ç»“æŸ
4. é€ä¸ªæ ‡è®°æµå¼å¤„ç†ä»£ç†æœ€ç»ˆç­”æ¡ˆ
5. å¸¦è¾“å‡ºçš„ä»£ç†ç»“æŸ


```python
async for event in agent_executor.astream_events(
    {"input": "where is the cat hiding? what items are in that location?"},
    version="v1",
):
    kind = event["event"]
    if kind == "on_chain_start":
        if (
            event["name"] == "Agent"
        ):
            print(
                f"Starting agent: {event['name']} with input: {event['data'].get('input')}"
            )
    elif kind == "on_chain_end":
        if (
            event["name"] == "Agent"
        ):
            print()
            print("--")
            print(
                f"Done agent: {event['name']} with output: {event['data'].get('output')['output']}"
            )
    if kind == "on_chat_model_stream":
        content = event["data"]["chunk"].content
        if content:
            # åœ¨OpenAIä¸Šä¸‹æ–‡ä¸­â€œç©ºå†…å®¹â€è¡¨ç¤ºæ¨¡å‹æ­£åœ¨è¯·æ±‚è°ƒç”¨å·¥å…·ã€‚
            # æ‰€ä»¥æˆ‘ä»¬åªæ‰“å°éç©ºå†…å®¹
            print(content, end="|")
    elif kind == "on_tool_start":
        print("--")
        print(
            f"Starting tool: {event['name']} with inputs: {event['data'].get('input')}"
        )
    elif kind == "on_tool_end":
        print(f"Done tool: {event['name']}")
        print(f"Tool output was: {event['data'].get('output')}")
        print("--")
```

```
    å¼€å§‹ä»£ç†: Agent with input: {'input': 'where is the cat hiding? what items are in that location?'}
    --
    å¼€å§‹å·¥å…·: where_cat_is_hiding with inputs: {}
    å®Œæˆå·¥å…·: where_cat_is_hiding
    å·¥å…·è¾“å‡ºä¸º: on the shelf
    --
    --
    å¼€å§‹å·¥å…·: get_items with inputs: {'place': 'shelf'}
    å®Œæˆå·¥å…·: get_items
    å·¥å…·è¾“å‡ºä¸º: books, pencils and pictures
    The| cat| is| currently| hiding| on| the| shelf|.| In| that| location|,| you| can| find| books|,| pencils|,| and| pictures|.|
    --
    å®Œæˆä»£ç†: Agent with output: The cat is currently hiding on the shelf. In that location, you can find books, pencils, and pictures.
```   

### ä»å·¥å…·å†…éƒ¨æµå¼å¤„ç†äº‹ä»¶

å¦‚æœä½ çš„å·¥å…·åˆ©ç”¨LangChainå¯è¿è¡Œå¯¹è±¡(ä¾‹å¦‚LCELé“¾ã€LLMsã€æ£€ç´¢å™¨ç­‰)ï¼Œ
å¹¶ä¸”ä½ æƒ³è¦ä»è¿™äº›å¯¹è±¡ä¸­æµå¼å¤„ç†äº‹ä»¶ï¼Œä½ éœ€è¦ç¡®ä¿å›è°ƒæ­£ç¡®ä¼ æ’­ã€‚

æŸ¥çœ‹å¦‚ä½•ä¼ é€’å›è°ƒï¼Œè®©æˆ‘ä»¬é‡æ–°å®ç°`get_items`å·¥å…·ï¼Œä½¿å…¶ä½¿ç”¨LLMå¹¶å°†å›è°ƒä¼ é€’ç»™LLMã€‚éšæ—¶æ ¹æ®è‡ªå·±çš„æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚


```python
@tool
async def get_items(place: str, callbacks: Callbacks) -> str:
    """ä½¿ç”¨æ­¤å·¥å…·æŸ¥æ‰¾ç»™å®šåœ°ç‚¹ä¸­çš„ç‰©å“ã€‚"""
    template = ChatPromptTemplate.from_messages(
        [
            (
                "human",
                "Can you tell me what kind of items i might find in the following place: '{place}'. "
                "List at least 3 such items separating them by a comma. And include a brief description of each item..",
            )
        ]
    )
    chain = template | model.with_config(
        {
            "run_name": "Get Items LLM",
            "tags": ["tool_llm"],
            "callbacks": callbacks,
        }
    )
    chunks = [chunk async for chunk in chain.astream({"place": place})]
    return "".join(chunk.content for chunk in chunks)
```

^çœ‹çœ‹å·¥å…·å¦‚ä½•ä¼ æ’­å›è°ƒã€‚

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬åˆå§‹åŒ–æˆ‘ä»¬çš„ä»£ç†ï¼Œå¹¶æŸ¥çœ‹æ–°çš„è¾“å‡ºã€‚


```python
prompt = hub.pull("hwchase17/openai-tools-agent")
tools = [get_items, where_cat_is_hiding]
agent = create_openai_tools_agent(
    model.with_config({"tags": ["agent_llm"]}), tools, prompt
)
agent_executor = AgentExecutor(agent=agent, tools=tools).with_config(
    {"run_name": "Agent"}
)

async for event in agent_executor.astream_events(
    {"input": "where is the cat hiding? what items are in that location?"},
    version="v1",
):
    kind = event["event"]
    if kind == "on_chain_start":
        if (
            event["name"] == "Agent"
        ):
            print(
                f"Starting agent: {event['name']} with input: {event['data'].get('input')}"
            )
    elif kind == "on_chain_end":
        if (
            event["name"] == "Agent"
        ):
            print()
            print("--")
            print(
                f"Done agent: {event['name']} with output: {event['data'].get('output')['output']}"
            )
    if kind == "on_chat_model_stream":
        content = event["data"]["chunk"].content
        if content:
            # åœ¨OpenAIä¸Šä¸‹æ–‡ä¸­â€œç©ºå†…å®¹â€è¡¨ç¤ºæ¨¡å‹æ­£åœ¨è¯·æ±‚è°ƒç”¨å·¥å…·ã€‚
            # æ‰€ä»¥æˆ‘ä»¬åªæ‰“å°éç©ºå†…å®¹
            print(content, end="|")
    elif kind == "on_tool_start":
        print("--")
        print(
            f"Starting tool: {event['name']} with inputs: {event['data'].get('input')}"
        )
    elif kind == "on_tool_end":
        print(f"Done tool: {event['name']}")
        print(f"Tool output was: {event['data'].get('output')}")
        print("--")
```

    å¼€å§‹ä»£ç†: Agent with input: {'input': 'where is the cat hiding? what items are in that location?'}
    --
    å¼€å§‹å·¥å…·: where_cat_is_hiding with inputs: {}
    å®Œæˆå·¥å…·: where_cat_is_hiding
    å·¥å…·è¾“å‡ºä¸º: on the shelf
    --
    --
    å¼€å§‹å·¥å…·: get_items with inputs: {'place': 'shelf'}
    åœ¨|ä¸€ä¸ª|æ¶å­|ä¸Š|,| ä½ |å¯èƒ½|ä¼š|æ‰¾åˆ°|:
    
    |1|.| å›¾ä¹¦|:| æ¶å­|é€šå¸¸|ç”¨æ¥|å­˜æ”¾|ä¹¦ç±|ã€‚| å®ƒ|å¯ä»¥|åŒ…å«|å„ç§|æµæ´¾|çš„|å°è¯´|ã€|æ•™ç§‘ä¹¦|æˆ–|å‚è€ƒä¹¦|ã€‚| å›¾ä¹¦|æä¾›|çŸ¥è¯†|ã€|å¨±ä¹|ï¼Œ|å¹¶é€šè¿‡|è®²æ•…äº‹|å°†|ä½ |å¸¦åˆ°|ä¸åŒ|çš„|ä¸–ç•Œ|ã€‚
    
    |2|.| è£…é¥°ç‰©å“|:| æ¶å­|ç»å¸¸|å±•ç¤º|è£…é¥°ç‰©å“|ï¼Œ|å¦‚|å°é›•åƒ|ã€|èŠ±ç“¶|æˆ–|ç…§ç‰‡æ¡†|ã€‚| è¿™äº›|ç‰©å“|ç»™|ç©ºé—´|å¢æ·»|ä¸ªäºº|é£æ ¼|ï¼Œ|å¯ä»¥|åæ˜ |æ‰€æœ‰è€…|çš„|å…´è¶£|æˆ–|å›å¿†|ã€‚
    
    |3|.| å‚¨ç‰©ç›’|:| æ¶å­|è¿˜|å¯ä»¥|æ”¾ç½®|å‚¨ç‰©ç›’|æˆ–|ç¯®å­|ã€‚| è¿™äº›|å®¹å™¨|å¸®åŠ©|ç»„ç»‡|å’Œ|æ¸…ç†|ç©ºé—´|ï¼Œ|å­˜æ”¾|æ–‡æ¡£|ã€|é…ä»¶|æˆ–|å°|å®¶å±…ç”¨å“|ç­‰|æ‚ç‰©|ã€‚| å®ƒä»¬|ä½¿|æ¶å­|çœ‹èµ·æ¥|æ•´æ´|å’Œ|æœ‰åº|ã€‚|
    å®Œæˆå·¥å…·: get_items
    å·¥å…·è¾“å‡ºä¸º: åœ¨ä¸€ä¸ªæ¶å­ä¸Š, ä½ å¯èƒ½ä¼šæ‰¾åˆ°:

    1. å›¾ä¹¦: æ¶å­é€šå¸¸ç”¨æ¥å­˜æ”¾ä¹¦ç±ã€‚å®ƒå¯ä»¥åŒ…å«å„ç§æµæ´¾çš„å°è¯´ã€æ•™ç§‘ä¹¦æˆ–å‚è€ƒä¹¦ã€‚å›¾ä¹¦æä¾›çŸ¥è¯†ã€å¨±ä¹ï¼Œå¹¶é€šè¿‡è®²æ•…äº‹å°†ä½ å¸¦åˆ°ä¸åŒçš„ä¸–ç•Œã€‚

    2. è£…é¥°ç‰©å“: æ¶å­ç»å¸¸å±•ç¤ºè£…é¥°ç‰©å“ï¼Œå¦‚å°é›•åƒã€èŠ±ç“¶æˆ–ç…§ç‰‡æ¡†ã€‚è¿™äº›ç‰©å“ç»™ç©ºé—´å¢æ·»ä¸ªäººé£æ ¼ï¼Œå¯ä»¥åæ˜ æ‰€æœ‰è€…çš„å…´è¶£æˆ–å›å¿†ã€‚

    3. å‚¨ç‰©ç›’: æ¶å­è¿˜å¯ä»¥æ”¾ç½®å‚¨ç‰©ç›’æˆ–ç¯®å­ã€‚è¿™äº›å®¹å™¨å¸®åŠ©ç»„ç»‡å’Œæ¸…ç†ç©ºé—´ï¼Œå­˜æ”¾æ–‡æ¡£ã€é…ä»¶æˆ–å°å®¶å±…ç”¨å“ç­‰æ‚ç‰©ã€‚å®ƒä»¬ä½¿æ¶å­çœ‹èµ·æ¥æ•´æ´å’Œæœ‰åºã€‚
    --
    çŒ«|æ­£åœ¨|æ¶å­|ä¸Š|èº²è—|ã€‚| åœ¨|é‚£|ä¸ª|ä½ç½®|,| ä½ |å¯èƒ½|ä¼š|æ‰¾åˆ°|ä¹¦ç±ã€è£…é¥°ç‰©å“å’Œ|å‚¨ç‰©ç›’|ã€‚|
    --
    å®Œæˆä»£ç†: Agent with output: çŒ«æ­£åœ¨æ¶å­ä¸Šèº²è—ã€‚åœ¨é‚£ä¸ªä½ç½®, ä½ å¯èƒ½ä¼šæ‰¾åˆ°ä¹¦ç±ã€è£…é¥°ç‰©å“å’Œå‚¨ç‰©ç›’ã€‚
    



### å…¶ä»–æ–¹æ³•

#### ä½¿ç”¨ astream_log

**æ³¨æ„** æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ [astream_log](/expression_language/interface#async-stream-intermediate-steps) APIã€‚è¯¥APIä¼šåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­äº§ç”Ÿæ‰€æœ‰äº‹ä»¶çš„è¯¦ç»†æ—¥å¿—ã€‚æ—¥å¿—æ ¼å¼åŸºäº [JSONPatch](https://jsonpatch.com/) æ ‡å‡†ã€‚å®ƒæ˜¯å…·æœ‰ç»†ç²’åº¦çš„æ—¥å¿—ï¼Œä½†éœ€è¦è§£æã€‚å‡ºäºè¿™ä¸ªåŸå› ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä»£æ›¿`astream_events` APIã€‚

```python
i = 0
async for chunk in agent_executor.astream_log(
    {"input": "çŒ«èº²åœ¨å“ªé‡Œï¼Ÿé‚£ä¸ªä½ç½®æœ‰ä»€ä¹ˆç‰©å“ï¼Ÿ"},
):
    print(chunk)
    i += 1
    if i > 10:
        break
```

    RunLogPatch({'op': 'replace',
      'path': '',
      'value': {'final_output': None,
                'id': 'c261bc30-60d1-4420-9c66-c6c0797f2c2d',
                'logs': {},
                'name': 'Agent',
                'streamed_output': [],
                'type': 'chain'}})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableSequence',
      'value': {'end_time': None,
                'final_output': None,
                'id': '183cb6f8-ed29-4967-b1ea-024050ce66c7',
                'metadata': {},
                'name': 'RunnableSequence',
                'start_time': '2024-01-22T20:38:43.650+00:00',
                'streamed_output': [],
                'streamed_output_str': [],
                'tags': [],
                'type': 'chain'}})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableAssign<agent_scratchpad>',
      'value': {'end_time': None,
                'final_output': None,
                'id': '7fe1bb27-3daf-492e-bc7e-28602398f008',
                'metadata': {},
                'name': 'RunnableAssign<agent_scratchpad>',
                'start_time': '2024-01-22T20:38:43.652+00:00',
                'streamed_output': [],
                'streamed_output_str': [],
                'tags': ['seq:step:1'],
                'type': 'chain'}})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableAssign<agent_scratchpad>/streamed_output/-',
      'value': {'input': 'çŒ«åœ¨å“ªé‡Œèº²è—ï¼Ÿé‚£ä¸ªä½ç½®æœ‰ä»€ä¹ˆç‰©å“ï¼Ÿ',
                'intermediate_steps': []}})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableParallel<agent_scratchpad>',
      'value': {'end_time': None,
                'final_output': None,
                'id': 'b034e867-e6bb-4296-bfe6-752c44fba6ce',
                'metadata': {},
                'name': 'RunnableParallel<agent_scratchpad>',
                'start_time': '2024-01-22T20:38:43.652+00:00',
                'streamed_output': [],
                'streamed_output_str': [],
                'tags': [],
                'type': 'chain'}})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableLambda',
      'value': {'end_time': None,
                'final_output': None,
                'id': '65ceef3e-7a80-4015-8b5b-d949326872e9',
                'metadata': {},
                'name': 'RunnableLambda',
                'start_time': '2024-01-22T20:38:43.653+00:00',
                'streamed_output': [],
                'streamed_output_str': [],
                'tags': ['map:key:agent_scratchpad'],
                'type': 'chain'}})
    RunLogPatch({'op': 'add', 'path': '/logs/RunnableLambda/streamed_output/-', 'value': []})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableParallel<agent_scratchpad>/streamed_output/-',
      'value': {'agent_scratchpad': []}})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableAssign<agent_scratchpad>/streamed_output/-',
      'value': {'agent_scratchpad': []}})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableLambda/final_output',
      'value': {'output': []}},
     {'op': 'add',
      'path': '/logs/RunnableLambda/end_time',
      'value': '2024-01-22T20:38:43.654+00:00'})
    RunLogPatch({'op': 'add',
      'path': '/logs/RunnableParallel<agent_scratchpad>/final_output',
      'value': {'agent_scratchpad': []}},
     {'op': 'add',
      'path': '/logs/RunnableParallel<agent_scratchpad>/end_time',
      'value': '2024-01-22T20:38:43.655+00:00'})
    

è¿™å¯èƒ½éœ€è¦ä¸€äº›é€»è¾‘æ‰èƒ½ä»¥å¯æ“ä½œçš„æ ¼å¼è·å¾—

```python
i = 0
path_status = {}
async for chunk in agent_executor.astream_log(
    {"input": "çŒ«èº²åœ¨å“ªé‡Œï¼Ÿé‚£ä¸ªä½ç½®æœ‰ä»€ä¹ˆç‰©å“ï¼Ÿ"},
):
    for op in chunk.ops:
        if op["op"] == "add":
            if op["path"] not in path_status:
                path_status[op["path"]] = op["value"]
            else:
                path_status[op["path"]] += op["value"]
    print(op["path"])
    print(path_status.get(op["path"]))
    print("----")
    i += 1
    if i > 30:
        break
```

    
    None
    ----
    /logs/RunnableSequence
    {'id': '22bbd5db-9578-4e3f-a6ec-9b61f08cb8a9', 'name': 'RunnableSequence', 'type': 'chain', 'tags': [], 'metadata': {}, 'start_time': '2024-01-22T20:38:43.668+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/RunnableAssign<agent_scratchpad>
    {'id': 'e0c00ae2-aaa2-4a09-bc93-cb34bf3f6554', 'name': 'RunnableAssign<agent_scratchpad>', 'type': 'chain', 'tags': ['seq:step:1'], 'metadata': {}, 'start_time': '2024-01-22T20:38:43.672+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/RunnableAssign<agent_scratchpad>/streamed_output/-
    {'input': 'çŒ«åœ¨å“ªé‡Œèº²è—ï¼Ÿé‚£ä¸ªä½ç½®æœ‰ä»€ä¹ˆç‰©å“ï¼Ÿ', 'intermediate_steps': []}
    ----
    /logs/RunnableParallel<agent_scratchpad>
    {'id': '26ff576d-ff9d-4dea-98b2-943312a37f4d', 'name': 'RunnableParallel<agent_scratchpad>', 'type': 'chain', 'tags': [], 'metadata': {}, 'start_time': '2024-01-22T20:38:43.674+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/RunnableLambda
    {'id': '9f343c6a-23f7-4a28-832f-d4fe3e95d1dc', 'name': 'RunnableLambda', 'type': 'chain', 'tags': ['map:key:agent_scratchpad'], 'metadata': {}, 'start_time': '2024-01-22T20:38:43.685+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/RunnableLambda/streamed_output/-
    []
    ----
    /logs/RunnableParallel<agent_scratchpad>/streamed_output/-
    {'agent_scratchpad': []}
    ----
    /logs/RunnableAssign<agent_scratchpad>/streamed_output/-
    {'input': 'çŒ«èº²åœ¨å“ªé‡Œï¼Ÿé‚£ä¸ªä½ç½®æœ‰ä»€ä¹ˆç‰©å“ï¼Ÿ', 'intermediate_steps': [], 'agent_scratchpad': []}
    ----
    /logs/RunnableLambda/end_time
    2024-01-22T20:38:43.687+00:00
    ----
    /logs/RunnableParallel<agent_scratchpad>/end_time
    2024-01-22T20:38:43.688+00:00
    ----
    /logs/RunnableAssign<agent_scratchpad>/end_time
    2024-01-22T20:38:43.688+00:00
    ----
    /logs/ChatPromptTemplate
    {'id': '7e3a84d5-46b8-4782-8eed-d1fe92be6a30', 'name': 'ChatPromptTemplate', 'type': 'prompt', 'tags': ['seq:step:2'], 'metadata': {}, 'start_time': '2024-01-22T20:38:43.689+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/ChatPromptTemplate/end_time
    2024-01-22T20:38:43.689+00:00
    ----
    /logs/ChatOpenAI
    {'id': '6446f7ec-b3e4-4637-89d8-b4b34b46ea14', 'name': 'ChatOpenAI', 'type': 'llm', 'tags': ['seq:step:3', 'agent_llm'], 'metadata': {}, 'start_time': '2024-01-22T20:38:43.690+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/ChatOpenAI/streamed_output/-
    content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_gKFg6FX8ZQ88wFUs94yx86PF', 'function': {'arguments': '', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]}
    ----
    /logs/ChatOpenAI/streamed_output/-
    content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_gKFg6FX8ZQ88wFUs94yx86PF', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]}
    ----
    /logs/ChatOpenAI/streamed_output/-
    content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_gKFg6FX8ZQ88wFUs94yx86PF', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]}
    ----
    /logs/ChatOpenAI/end_time
    2024-01-22T20:38:44.203+00:00
    ----
    /logs/OpenAIToolsAgentOutputParser
    {'id': '65912835-8dcd-4be2-ad05-9f239a7ef704', 'name': 'OpenAIToolsAgentOutputParser', 'type': 'parser', 'tags': ['seq:step:4'], 'metadata': {}, 'start_time': '2024-01-22T20:38:44.204+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/OpenAIToolsAgentOutputParser/end_time
    2024-01-22T20:38:44.205+00:00
    ----
    /logs/RunnableSequence/streamed_output/-
    [OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\nInvoking: `where_cat_is_hiding` with `{}`\n\n\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_gKFg6FX8ZQ88wFUs94yx86PF', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_gKFg6FX8ZQ88wFUs94yx86PF')]
    ----
    /logs/RunnableSequence/end_time
    2024-01-22T20:38:44.206+00:00
    ----
    /final_output
    None
    ----
    /logs/where_cat_is_hiding
    {'id': '21fde139-0dfa-42bb-ad90-b5b1e984aaba', 'name': 'where_cat_is_hiding', 'type': 'tool', 'tags': [], 'metadata': {}, 'start_time': '2024-01-22T20:38:44.208+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/where_cat_is_hiding/end_time
    2024-01-22T20:38:44.208+00:00
    ----
    /final_output/messages/1
    content='under the bed' name='where_cat_is_hiding'
    ----
    /logs/RunnableSequence:2
    {'id': '37d52845-b689-4c18-9c10-ffdd0c4054b0', 'name': 'RunnableSequence', 'type': 'chain', 'tags': [], 'metadata': {}, 'start_time': '2024-01-22T20:38:44.210+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/RunnableAssign<agent_scratchpad>:2
    {'id': '30024dea-064f-4b04-b130-671f47ac59bc', 'name': 'RunnableAssign<agent_scratchpad>', 'type': 'chain', 'tags': ['seq:step:1'], 'metadata': {}, 'start_time': '2024-01-22T20:38:44.213+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    /logs/RunnableAssign<agent_scratchpad>:2/streamed_output/-
    {'input': 'çŒ«èº²åœ¨å“ªé‡Œï¼Ÿé‚£ä¸ªä½ç½®æœ‰ä»€ä¹ˆç‰©å“ï¼Ÿ', 'intermediate_steps': [(OpenAIToolAgentAction(tool='where_cat_is_hiding', tool_input={}, log='\nInvoking: `where_cat_is_hiding` with `{}`\n\n\n', message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_gKFg6FX8ZQ88wFUs94yx86PF', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]})], tool_call_id='call_gKFg6FX8ZQ88wFUs94yx86PF'), 'under the bed')]}
    ----
    /logs/RunnableParallel<agent_scratchpad>:2
    {'id': '98906cd7-93c2-47e8-a7d7-2e8d4ab09ed0', 'name': 'RunnableParallel<agent_scratchpad>', 'type': 'chain', 'tags': [], 'metadata': {}, 'start_time': '2024-01-22T20:38:44.215+00:00', 'streamed_output': [], 'streamed_output_str': [], 'final_output': None, 'end_time': None}
    ----
    



#### ä½¿ç”¨å›è°ƒå‡½æ•°ï¼ˆå·²å¼ƒç”¨ï¼‰

æµå¤„ç†çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å›è°ƒå‡½æ•°ã€‚å¦‚æœæ‚¨ä»åœ¨è¿è¡Œè¾ƒæ—§ç‰ˆæœ¬çš„LangChainå¹¶ä¸”æ— æ³•å‡çº§ï¼Œåˆ™å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œè¿™å¹¶**ä¸**æ˜¯ä¸€ä¸ªæ¨èçš„æ–¹æ³•ï¼Œå› ä¸ºï¼š

1. å¯¹äºå¤§å¤šæ•°åº”ç”¨ç¨‹åºï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸¤ä¸ªå·¥ä½œè€…ï¼Œå°†å›è°ƒå†™å…¥é˜Ÿåˆ—ï¼Œå¹¶ä¸”è¿˜éœ€è¦å¦ä¸€ä¸ªå·¥ä½œè€…ä»é˜Ÿåˆ—ä¸­è¯»å–ï¼ˆå³ï¼Œéšè—äº†ä½¿å…¶æ­£å¸¸å·¥ä½œæ‰€éœ€çš„å¤æ‚æ€§ï¼‰ã€‚
2. **end** äº‹ä»¶å¯èƒ½ä¼šç¼ºå°‘ä¸€äº›å…ƒæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œåƒè¿è¡Œåç§°è¿™æ ·çš„ä¿¡æ¯ï¼‰ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨éœ€è¦é™„åŠ å…ƒæ•°æ®ï¼Œæ‚¨åº”è¯¥ç»§æ‰¿è‡ª `BaseTracer` è€Œä¸æ˜¯ `AsyncCallbackHandler`ï¼Œä»¥ä»è¿è¡Œï¼ˆå³è·Ÿè¸ªï¼‰ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œæˆ–è€…æ ¹æ® `run_id` è‡ªå·±å®ç°èšåˆé€»è¾‘ã€‚
3. å›è°ƒå‡½æ•°çš„è¡Œä¸ºä¸ä¸€è‡´ï¼ˆä¾‹å¦‚ï¼Œè¾“å…¥å’Œè¾“å‡ºçš„ç¼–ç æ–¹å¼ï¼‰ï¼Œå…·ä½“å–å†³äºæ‚¨éœ€è¦è§£å†³çš„å›è°ƒç±»å‹ã€‚

ä¸ºäº†è¯´æ˜é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢å®ç°äº†ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œæ˜¾ç¤ºäº†å¦‚ä½•è¿›è¡Œ *åŸºäºæ ‡è®°çš„* é€ä¸ªä»¤ç‰Œä¼ é€’ã€‚æ ¹æ®æ‚¨çš„åº”ç”¨ç¨‹åºéœ€æ±‚ï¼Œå¯ä»¥éšæ„å®ç°å…¶ä»–å›è°ƒå‡½æ•°ã€‚

ä½†æ˜¯ `astream_events` å·²ç»åœ¨å¹•åå¤„ç†äº†æ‰€æœ‰è¿™äº›ï¼Œå› æ­¤æ‚¨ä¸éœ€è¦ï¼

```python
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union
from uuid import UUID

from langchain_core.callbacks.base import AsyncCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult

# è¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰å¤„ç†ç¨‹åºï¼Œå°†ä»¤ç‰Œæ‰“å°åˆ°æ ‡å‡†è¾“å‡ºã€‚æ‚¨ä¹Ÿå¯ä»¥å°†æ•°æ®å‘é€åˆ°å…¶ä»–åœ°æ–¹ï¼Œä¾‹å¦‚æµå¼ API å“åº”


class TokenByTokenHandler(AsyncCallbackHandler):
    def __init__(self, tags_of_interest: List[str]) -> None:
        """è‡ªå®šä¹‰å›è°ƒå¤„ç†ç¨‹åºã€‚

        Args:
            tags_of_interest: åªä¼šæ‰“å°æ¥è‡ªå…·æœ‰è¿™äº›æ ‡ç­¾çš„æ¨¡å‹çš„ LLM ä»¤ç‰Œã€‚
        """
        self.tags_of_interest = tags_of_interest

    async def on_chain_start(
        self,
        serialized: Dict[str, Any],
        inputs: Dict[str, Any],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        """é“¾å¼€å§‹è¿è¡Œæ—¶è¿è¡Œã€‚"""
        print("é“¾å¼€å§‹ï¼š")
        print(inputs)

    async def on_chain_end(
        self,
        outputs: Dict[str, Any],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        """é“¾ç»“æŸè¿è¡Œæ—¶è¿è¡Œã€‚"""
        print("é“¾ç»“æŸ")
        print(outputs)

    async def on_chat_model_start(
        self,
        serialized: Dict[str, Any],
        messages: List[List[BaseMessage]],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> Any:
        """èŠå¤©æ¨¡å‹å¼€å§‹è¿è¡Œæ—¶è¿è¡Œã€‚"""
        overlap_tags = self.get_overlap_tags(tags)

        if overlap_tags:
            print(",".join(overlap_tags), end=": ", flush=True)

    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        inputs: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> Any:
        """å·¥å…·å¼€å§‹è¿è¡Œæ—¶è¿è¡Œã€‚"""
        print("å·¥å…·å¼€å§‹")
        print(serialized)

    def on_tool_end(
        self,
        output: Any,
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        **kwargs: Any,
    ) -> Any:
        """å·¥å…·ç»“æŸè¿è¡Œæ—¶è¿è¡Œã€‚"""
        print("å·¥å…·ç»“æŸ")
        print(str(output))

    async def on_llm_end(
        self,
        response: LLMResult,
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        """LLM ç»“æŸè¿è¡Œæ—¶è¿è¡Œã€‚"""
        overlap_tags = self.get_overlap_tags(tags)

        if overlap_tags:
            # è°ä¼šå¯¹ç¾ä¸½è¡¨ç¤ºæ€€ç–‘ï¼Ÿ
            print()
            print()

    def get_overlap_tags(self, tags: Optional[List[str]]) -> List[str]:
        """æ£€æŸ¥ä¸ç­›é€‰æ ‡è®°çš„é‡å éƒ¨åˆ†ã€‚"""
        if not tags:
            return []
        return sorted(set(tags or []) & set(self.tags_of_interest or []))

    async def on_llm_new_token(
        self,
        token: str,
        *,
        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> None:
        """è¿è¡Œæ–°çš„ LLM ä»¤ç‰Œã€‚ä»…åœ¨å¯ç”¨æµå¼ä¼ è¾“æ—¶å¯ç”¨ã€‚"""
        overlap_tags = self.get_overlap_tags(tags)

        if token and overlap_tags:
            print(token, end="|", flush=True)


handler = TokenByTokenHandler(tags_of_interest=["tool_llm", "agent_llm"])

result = await agent_executor.ainvoke(
    {"input": "where is the cat hiding and what items can be found there?"},
    {"callbacks": [handler]},
)
```

```
on chain start: 
{'input': 'where is the cat hiding and what items can be found there?'}
on chain start: 
{'input': ''}
on chain start: 
{'input': ''}
on chain start: 
{'input': ''}
on chain start: 
{'input': ''}
On chain end
[]
On chain end
{'agent_scratchpad': []}
On chain end
{'input': 'where is the cat hiding and what items can be found there?', 'intermediate_steps': [], 'agent_scratchpad': []}
on chain start: 
{'input': 'where is the cat hiding and what items can be found there?', 'intermediate_steps': [], 'agent_scratchpad': []}
On chain end
{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptValue'], 'kwargs': {'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'SystemMessage'], 'kwargs': {'content': 'You are a helpful assistant', 'additional_kwargs': {}}}, {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'schema', 'messages', 'HumanMessage'], 'kwargs': {'content': 'where is the cat hiding and what items can be found there?', 'additional_kwargs': {}}}]}}
agent_llm: 

on chain start: 
content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_pboyZTT0587rJtujUluO2OOc', 'function': {'arguments': '{}', 'name': 'where_cat_is_hiding'}, 'type': 'function'}]}
On chain end
...
On chain end
return_values={'output': 'The cat is hiding on the shelf. In the shelf, you might find books, decorative items, and storage boxes.'} log='The cat is hiding on the shelf. In the shelf, you might find books, decorative items, and storage boxes.'
On chain end
{'output': 'The cat is hiding on the shelf. In the shelf, you might find books, decorative items, and storage boxes.'}
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```


# æ•°æ®æµ


æ‰€æœ‰ChatModelséƒ½å®ç°äº†Runnableæ¥å£ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰æ–¹æ³•çš„é»˜è®¤å®ç°ï¼Œä¾‹å¦‚invokeã€batchã€abatchã€streamã€astreamã€‚è¿™ä¸ºæ‰€æœ‰ChatModelsæä¾›äº†åŸºæœ¬çš„æ•°æ®æµæ”¯æŒã€‚

æ•°æ®æµæ”¯æŒé»˜è®¤è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼ˆæˆ–è€…åœ¨å¼‚æ­¥æ•°æ®æµä¸­è¿”å›ä¸€ä¸ªAsyncIteratorï¼‰ï¼Œå…¶ä¸­åŒ…å«åº•å±‚ChatModelæä¾›ç¨‹åºè¿”å›çš„æœ€ç»ˆç»“æœã€‚è¿™æ˜¾ç„¶ä¸èƒ½æä¾›é€ä¸ªæ ‡è®°çš„æ•°æ®æµï¼Œè¿™éœ€è¦ChatModelæä¾›ç¨‹åºçš„åŸç”Ÿæ”¯æŒï¼Œä½†ç¡®ä¿æ‚¨çš„ä»£ç å¯ç”¨äºæˆ‘ä»¬ChatModelé›†æˆçš„ä»»ä½•è¿­ä»£å™¨æ ‡è®°ã€‚

æŸ¥çœ‹[æ”¯æŒé€ä¸ªæ ‡è®°æ•°æ®æµçš„é›†æˆæƒ…å†µ](/docs/integrations/chat/)ã€‚

```python
from langchain_community.chat_models import ChatAnthropic
```

```python
chat = ChatAnthropic(model="claude-2")
for chunk in chat.stream("ç»™æˆ‘å†™ä¸€é¦–å…³äºæœˆçƒä¸Šé‡‘é±¼çš„æ­Œ"):
    print(chunk.content, end="", flush=True)
```

è¿™æ˜¯æˆ‘åˆšåˆ›ä½œçš„ä¸€é¦–å…³äºæœˆçƒä¸Šé‡‘é±¼çš„æ­Œï¼š

æ¼‚æµ®åœ¨å¤ªç©ºä¸­ï¼Œå¯»æ‰¾ä¸€ä¸ªåœ°æ–¹
å«åšä»–ä»¬çš„å®¶ï¼Œå­¤ç‹¬ä¸€äºº
ç©¿è¿‡æ˜Ÿæ˜Ÿæ¸¸æ³³ï¼Œè¿™äº›æ¥è‡ªç«æ˜Ÿçš„é‡‘é±¼
æŠ›å¼ƒäº†ä»–ä»¬çš„é±¼ç¼¸ï¼Œå¯»æ‰¾æ–°çš„ç”Ÿæ´»
åœ¨æœˆçƒä¸Šï¼Œåœ¨é‚£äº›å‘æ´é‡Œ
å¯»æ‰¾é£Ÿç‰©ï¼Œä¹Ÿè®¸ä¸€äº›æœˆçƒé£Ÿç‰©
æ·±ä¸å¯æµ‹ï¼Œæ¥è¿‘æ­»äº¡
ä»–ä»¬å¤šä¹ˆå¸Œæœ›ï¼Œåªæœ‰ä¸€æ¡å°é±¼
åŠ å…¥ä»–ä»¬åœ¨è¿™é‡Œï¼Œåœ¨ä»–ä»¬çš„æœªæ¥çŠ¹è±«ä¸å†³
åœ¨æœˆçƒä¸Šï¼Œåœ¨åœ°çƒçš„èƒŒå
æ†§æ†¬ç€å®¶ï¼Œå……æ»¡æ³¡æ²«
ä»–ä»¬çš„èº«ä½“é€‚åº”ï¼Œç»§ç»­æŒç»­
åœ¨æœˆçƒä¸Šï¼Œåœ¨é‚£é‡Œä»–ä»¬å­¦ä¼šäº†æƒŠå¹
å¯¹å®‡èˆªå‘˜æˆå¼„çš„å¥¶é…ª
å½“ä»–ä»¬å‡è§†ç€åœ°çƒæ—¶ï¼Œè¿™ä¸ªè¯ç”Ÿä¹‹åœ°
è¿™äº›ç¦»å¼€æ°´ä¸­çš„é‡‘é±¼ï¼Œç»§ç»­æ¸¸å•Šæ¸¸
æœˆçƒä¸Šçš„å…ˆé©±è€…ï¼Œå¾æœä»–ä»¬çš„ææƒ§
åœ¨æœˆçƒä¸Šï¼Œåœ¨é‚£é‡Œä»–ä»¬å¿«ä¹åœ°æ­Œå”±




```